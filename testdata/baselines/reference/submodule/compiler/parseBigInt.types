//// [tests/cases/compiler/parseBigInt.ts] ////

=== parseBigInt.ts ===
// All bases should allow "n" suffix
const bin = 0b101, binBig = 0b101n; // 5, 5n
>bin : 5
>0b101 : 5
>binBig : 0b101n
>0b101n : 0b101n

const oct = 0o567, octBig = 0o567n; // 375, 375n
>oct : 375
>0o567 : 375
>octBig : 0o567n
>0o567n : 0o567n

const hex = 0xC0B, hexBig = 0xC0Bn; // 3083, 3083n
>hex : 3083
>0xC0B : 3083
>hexBig : 0xc0bn
>0xC0Bn : 0xc0bn

const dec = 123,   decBig = 123n;
>dec : 123
>123 : 123
>decBig : 123n
>123n : 123n

// Test literals whose values overflow a 53-bit integer
// These should be represented exactly in the emitted JS
const largeBin = 0b10101010101010101010101010101010101010101010101010101010101n; // 384307168202282325n
>largeBin : 0b10101010101010101010101010101010101010101010101010101010101n
>0b10101010101010101010101010101010101010101010101010101010101n : 0b10101010101010101010101010101010101010101010101010101010101n

const largeOct = 0o123456712345671234567n; // 1505852261029722487n
>largeOct : 0o123456712345671234567n
>0o123456712345671234567n : 0o123456712345671234567n

const largeDec = 12345678091234567890n;
>largeDec : 12345678091234567890n
>12345678091234567890n : 12345678091234567890n

const largeHex = 0x1234567890abcdefn; // 1311768467294899695n
>largeHex : 0x1234567890abcdefn
>0x1234567890abcdefn : 0x1234567890abcdefn

// Test literals with separators
const separatedBin = 0b010_10_1n; // 21n
>separatedBin : 0b010101n
>0b010_10_1n : 0b010101n

const separatedOct = 0o1234_567n; // 342391n
>separatedOct : 0o1234567n
>0o1234_567n : 0o1234567n

const separatedDec = 123_456_789n;
>separatedDec : 123456789n
>123_456_789n : 123456789n

const separatedHex = 0x0_abcdefn; // 11259375n
>separatedHex : 0x0abcdefn
>0x0_abcdefn : 0x0abcdefn

// Test parsing literals of different bit sizes
// to ensure that parsePseudoBigInt() allocates enough space
const zero         = 0b0n;
>zero : 0b0n
>0b0n : 0b0n

const oneBit       = 0b1n;
>oneBit : 0b1n
>0b1n : 0b1n

const twoBit       = 0b11n; // 3n
>twoBit : 0b11n
>0b11n : 0b11n

const threeBit     = 0b111n; // 7n
>threeBit : 0b111n
>0b111n : 0b111n

const fourBit      = 0b1111n; // 15n
>fourBit : 0b1111n
>0b1111n : 0b1111n

const fiveBit      = 0b11111n; // 31n
>fiveBit : 0b11111n
>0b11111n : 0b11111n

const sixBit       = 0b111111n; // 63n
>sixBit : 0b111111n
>0b111111n : 0b111111n

const sevenBit     = 0b1111111n; // 127n
>sevenBit : 0b1111111n
>0b1111111n : 0b1111111n

const eightBit     = 0b11111111n; // 255n
>eightBit : 0b11111111n
>0b11111111n : 0b11111111n

const nineBit      = 0b111111111n; // 511n
>nineBit : 0b111111111n
>0b111111111n : 0b111111111n

const tenBit       = 0b1111111111n; // 1023n
>tenBit : 0b1111111111n
>0b1111111111n : 0b1111111111n

const elevenBit    = 0b11111111111n; // 2047n
>elevenBit : 0b11111111111n
>0b11111111111n : 0b11111111111n

const twelveBit    = 0b111111111111n; // 4095n
>twelveBit : 0b111111111111n
>0b111111111111n : 0b111111111111n

const thirteenBit  = 0b1111111111111n; // 8191n
>thirteenBit : 0b1111111111111n
>0b1111111111111n : 0b1111111111111n

const fourteenBit  = 0b11111111111111n; // 16383n
>fourteenBit : 0b11111111111111n
>0b11111111111111n : 0b11111111111111n

const fifteenBit   = 0b111111111111111n; // 32767n
>fifteenBit : 0b111111111111111n
>0b111111111111111n : 0b111111111111111n

const sixteenBit   = 0b1111111111111111n; // 65535n
>sixteenBit : 0b1111111111111111n
>0b1111111111111111n : 0b1111111111111111n

const seventeenBit = 0b11111111111111111n; // 131071n
>seventeenBit : 0b11111111111111111n
>0b11111111111111111n : 0b11111111111111111n

// Test negative literals
const neg = -123n;
>neg : -123n
>-123n : -123n
>123n : 123n

const negHex: -16n = -0x10n;
>negHex : -16n
>-16n : -16n
>16n : 16n
>-0x10n : -0x10n
>0x10n : 0x10n

// Test normalization of bigints -- all of these should succeed
const negZero: 0n = -0n;
>negZero : 0n
>-0n : -0n
>0n : 0n

const baseChange: 255n = 0xFFn;
>baseChange : 255n
>0xFFn : 0xffn

const leadingZeros: 0xFFn = 0x000000FFn;
>leadingZeros : 0xffn
>0x000000FFn : 0x000000ffn

// Plus not allowed on literals
const unaryPlus = +123n;
>unaryPlus : number
>+123n : number
>123n : 123n

const unaryPlusHex = +0x123n;
>unaryPlusHex : number
>+0x123n : number
>0x123n : 0x123n

// Parsing errors
// In separate blocks because they each declare an "n" variable
{ const legacyOct = 0123n; }
>legacyOct : 123
>0123 : 123
>n : any

{ const scientific = 1e2n; }
>scientific : 100
>1e2n : 100

{ const decimal = 4.1n; }
>decimal : 4.1
>4.1n : 4.1

{ const leadingDecimal = .1n; }
>leadingDecimal : 0.1
>.1n : 0.1

const emptyBinary = 0bn; // should error but infer 0n
>emptyBinary : 0b0n
>0bn : 0b0n

const emptyOct = 0on; // should error but infer 0n
>emptyOct : 0o0n
>0on : 0o0n

const emptyHex = 0xn; // should error but infer 0n
>emptyHex : 0x0n
>0xn : 0x0n

const leadingSeparator = _123n;
>leadingSeparator : any
>_123n : any

const trailingSeparator = 123_n;
>trailingSeparator : 123n
>123_n : 123n

const doubleSeparator = 123_456__789n;
>doubleSeparator : 123456789n
>123_456__789n : 123456789n

// Using literals as types
const oneTwoOrThree = (x: 1n | 2n | 3n): bigint => x ** 2n;
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>(x: 1n | 2n | 3n): bigint => x ** 2n : (x: 1n | 2n | 3n) => bigint
>x : 1n | 2n | 3n
>x ** 2n : bigint
>x : 1n | 2n | 3n
>2n : 2n

oneTwoOrThree(0n); oneTwoOrThree(1n); oneTwoOrThree(2n); oneTwoOrThree(3n);
>oneTwoOrThree(0n) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>0n : 0n
>oneTwoOrThree(1n) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>1n : 1n
>oneTwoOrThree(2n) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>2n : 2n
>oneTwoOrThree(3n) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>3n : 3n

oneTwoOrThree(0);  oneTwoOrThree(1);  oneTwoOrThree(2);  oneTwoOrThree(3);
>oneTwoOrThree(0) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>0 : 0
>oneTwoOrThree(1) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>1 : 1
>oneTwoOrThree(2) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>2 : 2
>oneTwoOrThree(3) : bigint
>oneTwoOrThree : (x: 1n | 2n | 3n) => bigint
>3 : 3

